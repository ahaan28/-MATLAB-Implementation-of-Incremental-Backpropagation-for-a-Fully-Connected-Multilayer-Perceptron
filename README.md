This study presents the implementation and analysis of a fully connected neural network designed to process two training examples for a binary classification task, using a learning rate of 0.2. The network architecture comprises two input nodes, three hidden neurons with sigmoid activation functions, and a single output node. Initial weights are specified, with certain weights (w10 and w11) set to zero to establish full connectivity. The training process involves forward propagation to compute node outputs, error calculation, and backward propagation to update weights based on the calculated errors (betas) and weight changes (deltas). For the first training example (x1 = 0, x2 = 1, target = 1), the network computes hidden neuron outputs, final output, and errors, followed by weight updates. The process is repeated for the second training example (x1 = 1, x2 = 0, target = 1) using updated weights from the first iteration. Both manual calculations and MATLAB code are provided, with results verified to be consistent. The computations demonstrate the network's ability to learn from input data through iterative weight adjustments, showcasing the fundamental principles of neural network training via backpropagation.

